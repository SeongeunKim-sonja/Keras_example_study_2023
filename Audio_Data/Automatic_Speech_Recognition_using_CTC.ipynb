{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongeunKim-sonja/Keras_example_study_2023/blob/main/Audio_Data/Automatic_Speech_Recognition_using_CTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CTC를 이용한 음성 자동 인식 예제\n",
        "\n",
        "*이 문서는 keras 예제를 분석하여 해설을 작성한 것입니다.*\n",
        "\n",
        "<br>[예제 페이지](https://keras.io/examples/audio/ctc_asr/)\n",
        "<br>[CTC 참고](https://ratsgo.github.io/speechbook/docs/neuralam/ctc)\n",
        "<br>[음성인식 딥러닝 강의 영상](https://youtu.be/MghbHLuupoA)\n",
        "<br>[강의 영상 정리 글](https://lynnshin.tistory.com/42)\n",
        "\n",
        "<br>이 페이지의 예제 코드는 2D CNN과 RNN, CTC loss를 이용하여 음성 자동인식(ASR)을 구현한 것입니다. \n",
        "<br>데이터 셋은 LibriVox 프로젝트의 LJSSpeech 데이터셋(논픽션 책 7권의 문장을 녹음한 오디오 클립)을 사용하였습니다.\n",
        "<br>모델의 정확도를 판별하기 위해서는 WER(단어 오류율)을 사용하는데, 오류율의 계산을 위해 jiwer패키지를 설치합니다. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "knFAq6rtEgvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CTC\n",
        "input 데이터와 output 데이터 사이의 구간별 1대 1 라벨링이 되어있지 않을 때 사용하는 분류 기법.<br>\n",
        "CTC모델의 기본은 RNN기반 시퀀스모델 여러개와 softmax를 통과해서 출력함.<br>\n",
        "음소사이에는 묵음을 처리하는 입실론ε이 추가됨.<br><br>\n",
        "\n",
        "1. 구간을 나누어 음소를 판별\n",
        "![CTC1](https://i.imgur.com/hpVlJXr.png)\n",
        "<br><br>\n",
        "2. 입실론(묵음)구간 및 반복되는 음소 삭제\n",
        "![CTC2](https://i.imgur.com/LRjrS68.png)\n"
      ],
      "metadata": {
        "id": "Uzmj2ILHJHmf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "695fvAJSEVlY"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kmic9K1EhtH",
        "outputId": "2e9e4e92-bc92-45ab-daff-2badb9a47288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 56.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTw5gqhQEVlY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from jiwer import wer   #단어 오류율 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsyweKDuEVla"
      },
      "source": [
        "## Load the LJSpeech Dataset\n",
        "\n",
        "Let's download the [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/).\n",
        "The dataset contains 13,100 audio files as `wav` files in the `/wavs/` folder.\n",
        "The label (transcript) for each audio file is a string\n",
        "given in the `metadata.csv` file. The fields are:\n",
        "\n",
        "- **ID**: this is the name of the corresponding .wav file\n",
        "- **Transcription**: words spoken by the reader (UTF-8)\n",
        "- **Normalized transcription**: transcription with numbers,\n",
        "ordinals, and monetary units expanded into full words (UTF-8).\n",
        "\n",
        "For this demo we will use on the \"Normalized transcription\" field.\n",
        "\n",
        "Each audio file is a single-channel 16-bit PCM WAV with a sample rate of 22,050 Hz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DovrgQz3EVla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "f2bb05b8-8c61-44e7-fe03-fe591b33117f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "2748572632/2748572632 [==============================] - 28s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    file_name                           normalized_transcription\n",
              "0  LJ036-0013  found along the path of flight taken by the gu...\n",
              "1  LJ040-0227  Mrs. Siegel concluded her report with the stat...\n",
              "2  LJ014-0323  The warrant thus represented money, and was of..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb34681b-c7f3-4ecf-b4a1-50e691e2f17e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>normalized_transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LJ036-0013</td>\n",
              "      <td>found along the path of flight taken by the gu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LJ040-0227</td>\n",
              "      <td>Mrs. Siegel concluded her report with the stat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LJ014-0323</td>\n",
              "      <td>The warrant thus represented money, and was of...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb34681b-c7f3-4ecf-b4a1-50e691e2f17e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb34681b-c7f3-4ecf-b4a1-50e691e2f17e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb34681b-c7f3-4ecf-b4a1-50e691e2f17e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
        "data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)\n",
        "wavs_path = data_path + \"/wavs/\"\n",
        "metadata_path = data_path + \"/metadata.csv\"\n",
        "\n",
        "\n",
        "# Read metadata file and parse it\n",
        "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
        "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
        "metadata_df = metadata_df[[\"file_name\",\"normalized_transcription\"]]\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
        "metadata_df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g7MFhcFEVlb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c39bda7-35c8-4c3c-cf26-f685a6612b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the training set: 11790\n",
            "Size of the validation set: 1310\n"
          ]
        }
      ],
      "source": [
        "split = int(len(metadata_df) * 0.90)    #데이터셋을 9:1의 비율로 나누어 각각 학습 데이터와 테스트 데이터로 사용함\n",
        "df_train = metadata_df[:split]\n",
        "df_val = metadata_df[split:]\n",
        "\n",
        "print(f\"Size of the training set: {len(df_train)}\")\n",
        "print(f\"Size of the validation set: {len(df_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOzcZqCKEVlc"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We first prepare the vocabulary to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRkavm2EVlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd226077-658a-429e-f3f8-0e60d67d22f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"'\", '?', '!', ' '] (size =31)\n"
          ]
        }
      ],
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]    #리스트 컴프리헨션 list comprehension, 참고 https://wikidocs.net/22805\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")    \n",
        "#oov_token 설정한 경우에는 그 값을 index에 추가하여 out of vocabulary 인자를 대체한다. (예외처리)\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJohkL8lEVld"
      },
      "source": [
        "Next, we create the function that describes the transformation that we apply to each\n",
        "element of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCU2bcToEVld"
      },
      "outputs": [],
      "source": [
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 256\n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 160\n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
        "#제시되지 않은 경우 frame_length보다 크거나 같은, 가장 작은 2의 제곱수로 한다.\n",
        "fft_length = 384\n",
        "\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    #샘플을 인코딩 하는 과정, 스펙트로그램을 얻은 뒤 진폭값(절대값)만을 취하고, 표준화 하는 순서로 진행된다. \n",
        "    #영어는 대,소문자가 나뉘어 있으므로, 모두 소문자로 통일하여 case 값을 줄인다. \n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)  \n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    ###########################################\n",
        "    ##  Process the label\n",
        "    ##########################################\n",
        "    # 7. Convert label to Lower case\n",
        "    label = tf.strings.lower(label)\n",
        "    # 8. Split the label\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    # 9. Map the characters in label to numbers\n",
        "    label = char_to_num(label)\n",
        "    # 10. Return a dict as our model is expecting two inputs\n",
        "    return spectrogram, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh4av06BEVld"
      },
      "source": [
        "## Creating `Dataset` objects\n",
        "\n",
        "We create a `tf.data.Dataset` object that yields\n",
        "the transformed elements, in the same order as they\n",
        "appeared in the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHf6pS6nEVle"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "# Define the trainig dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n",
        ")\n",
        "train_dataset = (\n",
        "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Define the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n",
        ")\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGfETZ66EVle"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "Let's visualize an example in our dataset, including the\n",
        "audio clip, the spectrogram and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0RQ_I-dEVle"
      },
      "outputs": [],
      "source": [
        "#데이터 시각화\n",
        "#스펙트로그램과 라벨, 진폭을 그린 모양을 출력한다.\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "for batch in train_dataset.take(1):\n",
        "    spectrogram = batch[0][0].numpy()\n",
        "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
        "    label = batch[1][0]\n",
        "    # Spectrogram\n",
        "    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "    ax = plt.subplot(2, 1, 1)\n",
        "    ax.imshow(spectrogram, vmax=1)\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "    # Wav\n",
        "    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = audio.numpy()\n",
        "    ax = plt.subplot(2, 1, 2)\n",
        "    plt.plot(audio)\n",
        "    ax.set_title(\"Signal Wave\")\n",
        "    ax.set_xlim(0, len(audio))\n",
        "    display.display(display.Audio(np.transpose(audio), rate=16000))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1IRZTEmEVlf"
      },
      "source": [
        "## Model\n",
        "\n",
        "We first define the CTC Loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiLGpjDcEVlf"
      },
      "outputs": [],
      "source": [
        "#Loss함수로 사용될 CTCLoss 를 구현한다. \n",
        "def CTCLoss(y_true, y_pred):\n",
        "    # Compute the training-time loss value\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eDwsI93EVlf"
      },
      "source": [
        "We now define our model. We will define a model similar to\n",
        "[DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3vxUlZEEVlf"
      },
      "outputs": [],
      "source": [
        "#모델의 레이어를 구성한다. \n",
        "#Baidu 에서 발표한 DeepSpeech2(2015)의 모델과 유사하게 구성하였다.\n",
        "#Conv 2회 반복 후 RNN레이어를 반복문으로 구성한다. 이 모델에서는 5개의 RNN레이어가 사용되었다. \n",
        "#Optimizer는 Adam, Loss function은 위에서 선언한 CTCLoss 함수를 사용하였다. 이 덕분에 해당 모델에서는 디코더를 별도로 필요로 하지 않는다. \n",
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
        "    # Convolution layer 2\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 21],\n",
        "        strides=[1, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_2\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "        if i < rnn_layers:\n",
        "            x = layers.Dropout(rate=0.5)(x)\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    # Classification layer\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    # Model\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_units=512,\n",
        ")\n",
        "model.summary(line_length=110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSrlqdWaEVlg"
      },
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSKRfhMpEVlg"
      },
      "outputs": [],
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    # greedy search는 그때그때 확률이 높은 단어를 선택하기 때문에 연산 속도가 빠르지만 문법적으로 이상해지는 경우가 있음\n",
        "    # beam search는 미래를 고려하여 확률이 갖장 높은 조합 선택, 자연어 처리에서 성능을 높이기 위해 필수적이나 메모리 사용량과 계산양이 높다.\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "        wer_score = wer(targets, predictions)\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "        print(\"-\" * 100)\n",
        "        for i in np.random.randint(0, len(predictions), 2):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p_lwH0vEVlg"
      },
      "source": [
        "Let's start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uAQsV4gEVlg"
      },
      "outputs": [],
      "source": [
        "# Define the number of epochs. 에폭 수를 설정한다.\n",
        "epochs = 1\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[validation_callback],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcaMFIntEVlh"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ_1ErynEVlh"
      },
      "outputs": [],
      "source": [
        "# Let's check results on more validation samples\n",
        "#결과를 출력한다. 각 Epoch 마다 단어오류율WER을 표기하고, 원래의 문장과 예측한 문장을 차례로 출력한다. \n",
        "predictions = []\n",
        "targets = []\n",
        "for batch in validation_dataset:\n",
        "    X, y = batch\n",
        "    batch_predictions = model.predict(X)\n",
        "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "    predictions.extend(batch_predictions)\n",
        "    for label in y:\n",
        "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "        targets.append(label)\n",
        "wer_score = wer(targets, predictions)\n",
        "print(\"-\" * 100)\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "print(\"-\" * 100)\n",
        "for i in np.random.randint(0, len(predictions), 5):\n",
        "    print(f\"Target    : {targets[i]}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrrePEX9EVlh"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In practice, you should train for around 50 epochs or more. Each epoch\n",
        "takes approximately 5-6mn using a `GeForce RTX 2080 Ti` GPU.\n",
        "The model we trained at 50 epochs has a `Word Error Rate (WER) ≈ 16% to 17%`.\n",
        "\n",
        "해당 문서에서는 50 epoch진행 결과 WER 값이 16~17%정도로 낮아지는 것을 확인할 수 있었다고 한다. \n",
        "아래는 50 epoch의 결과를 출력한 것이다. \n",
        "\n",
        "Some of the transcriptions around epoch 50:\n",
        "\n",
        "**Audio file: LJ017-0009.wav**\n",
        "```\n",
        "- Target    : sir thomas overbury was undoubtedly poisoned by lord rochester in the reign\n",
        "of james the first\n",
        "- Prediction: cer thomas overbery was undoubtedly poisoned by lordrochester in the reign\n",
        "of james the first\n",
        "```\n",
        "\n",
        "**Audio file: LJ003-0340.wav**\n",
        "```\n",
        "- Target    : the committee does not seem to have yet understood that newgate could be\n",
        "only and properly replaced\n",
        "- Prediction: the committee does not seem to have yet understood that newgate could be\n",
        "only and proberly replace\n",
        "```\n",
        "\n",
        "**Audio file: LJ011-0136.wav**\n",
        "```\n",
        "- Target    : still no sentence of death was carried out for the offense and in eighteen\n",
        "thirtytwo\n",
        "- Prediction: still no sentence of death was carried out for the offense and in eighteen\n",
        "thirtytwo\n",
        "```\n",
        "\n",
        "Example available on HuggingFace.\n",
        "| Trained Model | Demo |\n",
        "| :--: | :--: |\n",
        "| [![Generic badge](https://img.shields.io/badge/🤗%20Model-CTC%20ASR-black.svg)](https://huggingface.co/keras-io/ctc_asr) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-CTC%20ASR-black.svg)](https://huggingface.co/spaces/keras-io/ctc_asr) |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#확장\n",
        "\n",
        "- 한국어 음성의 경우에는 어떤 식으로 분류하는 것이 좋을까?<br>\n",
        "  github sooftware의 kospeech 위키 문서에 따르면, 기존에 예상했던 대로 데이터셋에 존재하는 모든 음소(2337개)를 학습한 것으로 보인다.<br> [github wiki 문서](https://github.com/sooftware/KoSpeech/wiki/Preparation-before-Training)\n",
        "  <br>\n",
        "  '칠십 퍼센트' 문장의 경우 \n",
        "  '318 119 0 489 551 156' 와 같이 출력된다. \n",
        "  <br>\n",
        "  이때 0은 공백문자이다.<br>\n",
        "  낮은 단어오류율을 위해 충분히 많은 데이터셋을 필요로 할 것으로 보인다.\n",
        "  <br>\n",
        "  이외에도 Hybrid CTC-Attention을 이용한 음성 인식 모델의 경우 각각 자음과 모음을 음소로 분리하여 49개 자소를 사용하기도 하였다.\n",
        "\n",
        "  [참고 논문 링크](https://koreascience.kr/article/CFKO201832073078862.pdf)\n",
        "  <br><br>\n",
        "- 기존에 발표된 Deep speech2는 Baidu에서 발표하여 중국어 음성을 인식하는 모델인 것 같은데, 중국어는 발음기호(병음) 표기가 로마자로 이루어져 비교적 수월해보인다. <br>다만 동일한 병음이어도 성조가 다른 경우 어떻게 처리하는지 추후 확인해보면 좋을 것 같다. <br> -> Deep speech2의 경우 중국어 특성을 반영하여 가장 많이 사용하는 6,000개의 문자를 출력으로 정의하였다고 한다.<br>\n",
        "[Deep speech2 논문 링크](https://arxiv.org/abs/1512.02595)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O8uNTDKb1_gZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}