{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmJIY5avIdF0hJUtpla7jU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongeunKim-sonja/Keras_example_study_2023/blob/main/Video/Video_Classification_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer를 이용한 비디오 분류 - 뒷부분 추가중\n",
        "\n",
        "*이 문서는 keras 예제를 분석하여 해설을 작성한 것입니다.*\n",
        "\n",
        "<br>[예제 페이지](https://keras.io/examples/vision/video_transformers/)\n",
        "<br>[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "이 문서에서는 CNN-RNN 모델을 이용한 비디오 분류의 후속으로, Transformer 기반의 모델을 이용하여 비디오를 분류한다. \n",
        "\n",
        "\n",
        "이 예제는 TensorFlow 2.5 이상과 TensorFlow Docs를 필요로 한다.\n"
      ],
      "metadata": {
        "id": "LgbB87TbcwoL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv9f_SRzclUC"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data\n",
        "\n",
        "UCF101 데이터셋 : UCF101 - Action Recognition Data Set\n",
        "<br>[데이터셋 정보](https://www.crcv.ucf.edu/data/UCF101.php)\n",
        "<br>101개 동작으로 분류된 13320개의 비디오. \n",
        "<br>펀치, 자전거타기 등과 같은 동장으로 분류되어 있으며, 사람의 동작을 인식하는 모델을 생성하는 데에 사용된다. "
      ],
      "metadata": {
        "id": "K7y90qXZfdES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
        "!tar xf ucf101_top5.tar.gz"
      ],
      "metadata": {
        "id": "1jYGe7Wahfdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "4VeY6Ok4epCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ],
      "metadata": {
        "id": "8QMBBktghgtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define hyperparameters"
      ],
      "metadata": {
        "id": "vzW_Kim3hl9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "FU-96WvnhoHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preparation\n",
        "\n",
        "- 연산 속도 향상을 위해 224x224크기의 이미지를 128x128 크기로 축소한다.\n",
        "- 특징 추출을 위해 InceptionV3 네트워크 대신 DenseNet121을 사용한다.  \n",
        "\n",
        "#DenseNet121\n",
        "\n",
        "Keras 에서 제공하는 Pre-trained DenseNet model.\n",
        "<br> 121개의 layer를 가지고 있는 DenseNet을 말한다. \n",
        "<br>[DenseNet 참고 블로그](https://gaussian37.github.io/dl-concept-densenet/)\n",
        "\n",
        "\n",
        "<br>*DenseNet 관련하여 추가하면 좋을 것 같다.*"
      ],
      "metadata": {
        "id": "oBaJ5Pa1iMAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped\n",
        "\n",
        "\n",
        "# Following method is modified from this tutorial:\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "# 다음의 메서드는 위의 tensorflow 에서 제공하는 Inflated 3D Convnet 튜토리얼을 수정한 것이다. \n",
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "#DemseNet121(feature 추출을 위한) 정의의\n",
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.DenseNet121(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.densenet.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "# StringLookup 함수를 이용해 라벨링을 진행\n",
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "    labels = df[\"tag\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_features` are what we will feed to our sequence model.\n",
        "    # 'frame_features' 를 시퀀스 모델에 입력할 예정정\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "\n",
        "        # Pad shorter videos.\n",
        "        # 제로 프레임을 영상에 덧댄다.\n",
        "        if len(frames) < MAX_SEQ_LENGTH:\n",
        "            diff = MAX_SEQ_LENGTH - len(frames)\n",
        "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
        "            frames = np.concatenate(frames, padding)\n",
        "\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholder to store the features of the current video.\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                if np.mean(batch[j, :]) > 0.0:\n",
        "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                        batch[None, j, :]\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    temp_frame_features[i, j, :] = 0.0\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "\n",
        "    return frame_features, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRsjBNhT0gQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_df와 test_df에 prepare_all_videos()를 호출하면 완료되기까지 약 20분이 소요되므로, 시간을 절약하기 위해 미리 처리된 NumPy 배열을 다운로드 한다. "
      ],
      "metadata": {
        "id": "NA-edIwZ7Y1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n",
        "!tar xf top5_data_prepared.tar.gz"
      ],
      "metadata": {
        "id": "gGx0VthS7R2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.load(\"train_data.npy\"), np.load(\"train_labels.npy\")\n",
        "test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data.shape}\")"
      ],
      "metadata": {
        "id": "9nAASZdk701K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the Transformer-based model"
      ],
      "metadata": {
        "id": "aNDe8HW97-WP"
      }
    }
  ]
}